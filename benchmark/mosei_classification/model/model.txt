Model(
  (feature_extractors): ModuleDict(
    (text): TextExtractor(
      (gru): GRU(300, 30)
    )
    (audio): AudioExtractor(
      (gru): GRU(74, 30)
    )
    (vision): VisionExtractor(
      (gru): GRU(35, 30)
    )
  )
  (projectors): ModuleDict(
    (text): TextProjector(
      (ln): Linear(in_features=1500, out_features=60, bias=True)
    )
    (vision): VisionProjector(
      (ln): Linear(in_features=1500, out_features=60, bias=True)
    )
    (text+vision): MultimodalProjector(
      (ln): Linear(in_features=60, out_features=60, bias=True)
    )
  )
  (multimodal_transformer): MultimodalTransformer(
    (proj_l): Conv1d(300, 30, kernel_size=(1,), stride=(1,), bias=False)
    (trans_l_with_v): TransformerEncoder(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (layer_norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
    )
    (trans_l_mem): TransformerEncoder(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (layer_norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
    )
    (proj_v): Conv1d(35, 30, kernel_size=(1,), stride=(1,), bias=False)
    (trans_v_with_l): TransformerEncoder(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (layer_norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
    )
    (trans_v_mem): TransformerEncoder(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=30, out_features=30, bias=True)
          )
          (fc1): Linear(in_features=30, out_features=120, bias=True)
          (fc2): Linear(in_features=120, out_features=30, bias=True)
          (layer_norms): ModuleList(
            (0): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
            (1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (layer_norm): LayerNorm((30,), eps=1e-05, elementwise_affine=True)
    )
  )
  (regressor): Regressor(
    (ln): Linear(in_features=60, out_features=1, bias=True)
  )
  (L1Loss): L1Loss()
)